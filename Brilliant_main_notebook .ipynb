{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZvE0M1hcvo2jh2CH2b0odt6Z9n1KgPHp",
      "authorship_tag": "ABX9TyPMkReqzFHi46EmDCo/WnnZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanB1421/DATA601/blob/main/Brilliant_main_notebook%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA601 Project 1\n",
        "\n",
        "- Name: Daniel Brilliant\n",
        "- Date: November 11, 2022\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3PEPLjzwu8vR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following python program is developed from a class called FilePro. It is able to analyze folder_1 as an object using three different methods. These methods involve several imported python libraries, namely os, glob, re, hashlib, datetime, json, numpy, pandas, and treelib. The program uses three methods outside of the object constructor: get_info, generate_report, and tree_diagram. Each method produces a deliverable with information about folder_1 in different ways. The following is the code that developed the program using each of the aforementioned libraries, followed by demonstrations of the functionality of each non-constructor method."
      ],
      "metadata": {
        "id": "oSpXHcyBtlR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "73rkXKUzWrDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob"
      ],
      "metadata": {
        "id": "Buu0MU2kVKjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "cFtLRgWNtmlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib"
      ],
      "metadata": {
        "id": "Bk1ZKRBz1up1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime"
      ],
      "metadata": {
        "id": "wXFLKhpLwPH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "HQNO9pGd5LYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "bCjeSGJVJn8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "HvjbUBs1Ga8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install treelib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFp5otDBSXfo",
        "outputId": "dbfb9b82-937b-4ae7-aac4-caefecf71cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: treelib in /usr/local/lib/python3.7/dist-packages (1.6.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from treelib) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from treelib import Node, Tree"
      ],
      "metadata": {
        "id": "7sHreexJALkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder_1 = '/content/drive/MyDrive/folder_1' # stores folder_1 filepath in folder_1 variable"
      ],
      "metadata": {
        "id": "9siZnRFk_uyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHKxmMw6MVgL"
      },
      "outputs": [],
      "source": [
        "class FilePro: # python class FilePro created\n",
        "    def __init__(self, folder_1): # constructor method to create folder_1 object\n",
        "        self.folder_1 = folder_1 # initializes object to folder_1 filepath\n",
        "        os.chdir(self.folder_1) # changes the directory to folder_1\n",
        "        if os.path.exists('/content/file_info.json') == True: # checks if file_info.json exists\n",
        "            modtimestamp = os.path.getmtime('/content/file_info.json') # determines timestamp of modification for file_info.json\n",
        "            moddatetime = datetime.date.fromtimestamp(modtimestamp) # determines date of modification for file_info.json from timestamp\n",
        "            currentdate = datetime.date.today() # determines current date\n",
        "            datediff = moddatetime - currentdate # determines difference between the date of modification for file_info.json and the current date\n",
        "            if datediff.days < 2: # checks if the difference between the date of modification for file_info.json and the current date is less than two\n",
        "                with open('/content/file_info.json', 'r') as fp:\n",
        "                    self.record_list = json.load(fp) # loads record_list from file_info.json\n",
        "            else:\n",
        "                self.get_info() # runs get_info method if the difference between the date of modification for file_info.json and the current date is greater than two\n",
        "        else:\n",
        "            self.get_info() # runs get_info method if file_info.json is not present\n",
        "\n",
        "    def get_info(self): # method to determine important information for all files within folder_1 and store in a file\n",
        "        self.l1 = glob.glob('**.*') # stores first level files into l1\n",
        "        self.l2 = glob.glob('**/**.*') # stores second level files into l2\n",
        "        self.l3 = glob.glob('**/**/**.*') # stores third level files into l3\n",
        "        self.l4 = glob.glob('**/**/**/**.*') # stores fourth level files into l4\n",
        "        cwd_path = os.getcwd() # stores the current working directory as cwd_path\n",
        "        self.record_list = [] # creates list of records to be populated\n",
        "        for file in self.l1: # runs function for first level of folder_1\n",
        "            file_dict = {} # creates dictionary for each file\n",
        "            a = re.findall('[a-zA-Z0-9_-]+(?=)', file) # finds Filename and File Extension using regex findall function and stores in list a ([Filename, File Extension])\n",
        "            file_dict[\"File Extension\"] = a[1] # stores File Extension in dictionary (a[1] from list a)\n",
        "            file_size = os.path.getsize(file) # finds File Size\n",
        "            file_dict[\"File Size\"] = file_size # stores File Size in dictionary\n",
        "            f = open(cwd_path + '/' + file,'rb') # opens binary filepath\n",
        "            i = f.read() # reads file in binary\n",
        "            md5_value = hashlib.md5(i).hexdigest() # determins MD5 Checksum Value from hashlib function and binary reading\n",
        "            file_dict[\"MD5 Checksum Value\"] = md5_value # stores MD5 Checksum Value in dictionary\n",
        "            f.close() # closes binary filepath\n",
        "            file_dict[\"Filename\"] = a[0] # stores Filename in dictionary (a[0] from list a)\n",
        "            filepath = cwd_path + \"/\" + file # creates full Filepath as string\n",
        "            file_dict[\"Filepath\"] = filepath # stores Filepath in dictionary\n",
        "            timestamp = os.path.getmtime(file) # gets modification timestamp from file\n",
        "            datestamp = datetime.date.fromtimestamp(timestamp) # gets modification datetime from timestamp\n",
        "            date_stamp = datestamp.strftime(\"%m/%d/%Y\") # converts datetime into Modified Date string\n",
        "            file_dict[\"Modified Date\"] = date_stamp # stores Modified Date in dictionary\n",
        "            self.record_list.append(file_dict) # adds record dictionary for each file in level 1 to record_list\n",
        "        for file in self.l2: # runs function for second level of folder_1\n",
        "            file_dict = {} # creates dictionary for each file\n",
        "            a = re.findall('[a-zA-Z0-9_-]+(?=)', file) # finds Folder, Filename and File Extension using regex findall function and stores in list a ([Folder, Filename, File Extension])\n",
        "            file_dict[\"File Extension\"] = a[2] # stores File Extension in dictionary (a[2] from list a)\n",
        "            file_size = os.path.getsize(file) # finds File Size\n",
        "            file_dict[\"File Size\"] = file_size # stores File Size in dictionary\n",
        "            f = open(cwd_path + '/' + file,'rb') # opens binary filepath\n",
        "            i = f.read() # reads file in binary\n",
        "            md5_value = hashlib.md5(i).hexdigest() # determins MD5 Checksum Value from hashlib function and binary reading\n",
        "            file_dict[\"MD5 Checksum Value\"] = md5_value # stores MD5 Checksum Value in dictionary\n",
        "            f.close() # closes binary filepath\n",
        "            file_dict[\"Filename\"] = a[1] # stores Filename in dictionary (a[1] from list a)\n",
        "            filepath = cwd_path + \"/\" + file # creates full Filepath as string\n",
        "            file_dict[\"Filepath\"] = filepath # stores Filepath in dictionary\n",
        "            timestamp = os.path.getmtime(file) # gets modification time from file\n",
        "            datestamp = datetime.date.fromtimestamp(timestamp) # gets modification datetime from timestamp\n",
        "            date_stamp = datestamp.strftime(\"%m/%d/%Y\") # converts datetime into Modified Date string\n",
        "            file_dict[\"Modified Date\"] = date_stamp # stores Modified Date in dictionary\n",
        "            self.record_list.append(file_dict) # adds record dictionary for each file in level 2 to record_list\n",
        "        for file in self.l3: # runs function for third level of folder_1\n",
        "            file_dict = {} # creates dictionary for each file\n",
        "            a = re.findall('[a-zA-Z0-9_-]+(?=)', file) # finds Folder, Subfolder, Filename and File Extension using regex findall function and stores in list a ([Folder, Subfolder, Filename, File Extension])\n",
        "            file_dict[\"File Extension\"] = a[3] # stores File Extension in dictionary (a[3] from list a)\n",
        "            file_size = os.path.getsize(file) # finds File Size\n",
        "            file_dict[\"File Size\"] = file_size # stores File Size in dictionary\n",
        "            f = open(cwd_path + '/' + file,'rb') # opens binary filepath\n",
        "            i = f.read() # reads file in binary\n",
        "            md5_value = hashlib.md5(i).hexdigest() # determins MD5 Checksum Value from hashlib function and binary reading\n",
        "            file_dict[\"MD5 Checksum Value\"] = md5_value # stores MD5 Checksum Value in dictionary\n",
        "            f.close() # closes binary filepath\n",
        "            file_dict[\"Filename\"] = a[2] # stores Filename in dictionary (a[2] from list a)\n",
        "            filepath = cwd_path + \"/\" + file # creates full Filepath as string\n",
        "            file_dict[\"Filepath\"] = filepath # stores Filepath in dictionary\n",
        "            timestamp = os.path.getmtime(file) # gets modification timestamp from file\n",
        "            datestamp = datetime.date.fromtimestamp(timestamp) # gets modification datetime from timestamp\n",
        "            date_stamp = datestamp.strftime(\"%m/%d/%Y\") # converts datetime into Modified Date string\n",
        "            file_dict[\"Modified Date\"] = date_stamp # stores Modified Date in dictionary\n",
        "            self.record_list.append(file_dict) # adds record dictionary for each file in level 3 to record_list\n",
        "        for file in self.l4: # runs function for fourth level of folder_1\n",
        "            file_dict = {} # creates dictionary for each file\n",
        "            a = re.findall('[a-zA-Z0-9_-]+(?=)', file) # finds Folder, Subfolder, Subsubfolder, Filename and File Extension using regex findall function and stores in list a ([Folder, Subfolder, Subsubfolder, Filename, File Extension])\n",
        "            file_dict[\"File Extension\"] = a[4] # stores File Extension in dictionary (a[4] from list a)\n",
        "            file_size = os.path.getsize(file) # finds File Size\n",
        "            file_dict[\"File Size\"] = file_size # stores File Size in dictionary\n",
        "            f = open(cwd_path + '/' + file,'rb') # opens binary filepath\n",
        "            i = f.read() # reads file in binary\n",
        "            md5_value = hashlib.md5(i).hexdigest() # determins MD5 Checksum Value from hashlib function and binary reading\n",
        "            file_dict[\"MD5 Checksum Value\"] = md5_value  # stores MD5 Checksum Value in dictionary\n",
        "            f.close() # closes binary filepath\n",
        "            file_dict[\"Filename\"] = a[3] # stores Filename in dictionary (a[3] from list a)\n",
        "            filepath = cwd_path + \"/\" + file # creates full Filepath as string\n",
        "            file_dict[\"Filepath\"] = filepath # stores Filepath in dictionary\n",
        "            timestamp = os.path.getmtime(file) # gets modification timestamp from file\n",
        "            datestamp = datetime.date.fromtimestamp(timestamp) # gets modification datetime from timestamp\n",
        "            date_stamp = datestamp.strftime(\"%m/%d/%Y\") # converts datetime into Modified Date string\n",
        "            file_dict[\"Modified Date\"] = date_stamp # stores Modified Date in dictionary\n",
        "            self.record_list.append(file_dict) # adds record dictionary for each file in level 4 to record_list\n",
        "        with open('/content/file_info.json', 'w') as fp:\n",
        "            json.dump(self.record_list, fp) # writes record_list into json file named file_info.json\n",
        "    def generate_report(self): # method to generate statistical report derive from information stored within the file created in the get_info method\n",
        "        report_list = [] # creates a list to store the required statistics for the report\n",
        "        file_extension = [file['File Extension'] for file in self.record_list] # creates list of file extensions found in record_list\n",
        "        unique_file_extension = self.unique(file_extension) # pares down file extension list to unique file extensions\n",
        "        file_size = [file['File Size'] for file in self.record_list] # creates list of file sizes found in record_list\n",
        "        all_files_mean = np.mean(file_size) # finds the mean file size of all files in folder_1\n",
        "        all_files_median = np.median(file_size) # finds the median size of all files in folder_1\n",
        "        mean_of_files_dict = {'Mean Size of All Files (in bytes)': all_files_mean} # stores the mean file size in a dictionary with its labeled key\n",
        "        report_list.append(mean_of_files_dict) # adds the mean file size dictionary to report_list\n",
        "        median_of_files_dict = {'Median Size of All Files (in bytes)': all_files_median} # stores the mean file size in a dictionary with its labeled key\n",
        "        report_list.append(median_of_files_dict) # adds the median file size dictionary to report_list\n",
        "        file_extension_and_size = [[file['File Extension'], file['File Size']] for file in self.record_list] # creates list of lists containing the file extensions and file sizes from record_list\n",
        "        filesize_dict = {} # creates dictionary to store file sizes and organize them with their file extensions as keys\n",
        "        for extension in unique_file_extension:\n",
        "            filesize_dict[extension] = [] # stores unique file extensions as keys in filesize_dict and creates empty value lists where the file sizes will be stored\n",
        "        for sublist in file_extension_and_size:\n",
        "            filesize_dict[sublist[0]].append(sublist[1]) # stores file sizes (sublist[1] in filesize) in the empty value lists of filesize_dict based on their file extensions (sublist[0] in filesize)\n",
        "        for extension, size in filesize_dict.items(): # iterates over keys (file extensions) and values (file sizes) from filesize_dict converted into tuple pairs\n",
        "            filetype_mean = np.mean(size) # finds the mean file size for each unique file extension stored in filesize_dict\n",
        "            filetype_median = np.median(size) # finds the median file size for each unique file extension stored in filesize_dict\n",
        "            filetype_mean_dict = {extension + ' Mean Size (in bytes)': filetype_mean} # stores the mean file size value for each unique file extension in a dictionary with extension + 'Mean' as its key\n",
        "            filetype_median_dict = {extension + ' Median Size (in bytes)': filetype_median}  # stores the median file size value for each unique file extension in a dictionary with extension + 'Median' as its key\n",
        "            report_list.append(filetype_mean_dict) # adds the mean file size dictionary for each unique extension to report_list\n",
        "            report_list.append(filetype_median_dict) # adds the median file size dictionary for each unique extension to report_list\n",
        "        file_path = [file['Filepath'] for file in self.record_list] # creates list of filepaths found in record_list\n",
        "        file_name = [file['Filename'] for file in self.record_list] # creates list of filenames found in record_list\n",
        "        folders = [] # creates empty list of folders to be populated with the paths to each subfolder\n",
        "        for file in file_path: # iterates over each filepath\n",
        "            folder = os.path.dirname(file) # isolates the path to each folder found within the full filepaths\n",
        "            folders.append(folder) # stores the folder paths in folders\n",
        "        unique_folders = self.unique(folders) # pares down folder list to unique folderss\n",
        "        folder_number_dict = {} # creates a dictionary to store the files within each folder and organize them with their folders as keys\n",
        "        for folder in unique_folders:\n",
        "            folder_number_dict[folder] = [] # stores unique folders as keys in folder_number_dict and creates empty value lists where the filenames will be stored\n",
        "        folder_number_list = [list(file) for file in zip(folders, file_name)] # stores folders and filenames in a nested list\n",
        "        for sublist in folder_number_list:\n",
        "            folder_number_dict[sublist[0]].append(sublist[1]) # stores filenames (sublist[1] in folder_number_list) in the empty value lists of folder_number_dict based on their folders (sublist[0] in folder_number_list)\n",
        "        file_number_in_folder = {folder:len(filenames) for folder, filenames in folder_number_dict.items()} # creates new dictionary from folder_number_dict using the folders as keys and storing the number of files in each folder as the values\n",
        "        biggest_folder_number = max(file_number_in_folder, key = file_number_in_folder.get) # finds the folder with the most files in file_number_in_folder\n",
        "        biggest_folder_number_dict = {'Largest Folder by Number of Files': biggest_folder_number} # stores the folder with the most files in a dictionary with its labeled key\n",
        "        report_list.append(biggest_folder_number_dict) # adds the dictionary with the largest folder by number of files to report_list\n",
        "        folder_size_dict = {} # creates a dictionary to store the sizes of files within each folder and organize them with their folders as keys\n",
        "        for folder in unique_folders:\n",
        "            folder_size_dict[folder] = [] # stores unique folders as keys in folder_size_dict and creates empty value lists where the file sizes will be stored\n",
        "        folder_size_list = [list(file) for file in zip(folders, file_size)] # stores folders and file sizes in a nested list\n",
        "        for sublist in folder_size_list:\n",
        "            folder_size_dict[sublist[0]].append(sublist[1]) # stores file sizes (sublist[1] in folder_size_list) in the empty value lists of folder_size_dict based on their folders (sublist[0] in folder_size_list)\n",
        "        file_size_in_folder = {folder:sum(filesizes) for folder, filesizes in folder_size_dict.items()} # creates new dictionary from folder_size_dict using the folders as keys and storing the total size of all files in each folder as the values\n",
        "        biggest_folder_size = max(file_size_in_folder, key = file_size_in_folder.get) # finds the folder with the largest size in file_size_in_folder\n",
        "        biggest_folder_size_dict = {'Largest Folder by Combined File Size': biggest_folder_size} # stores the folder with the largest size in a dictionary with its labeled key\n",
        "        report_list.append(biggest_folder_size_dict) # adds the dictionary with the largest folder by total size to report_list\n",
        "        report_df = pd.DataFrame([values for dicts in report_list for values in dicts.items()], columns=[\"Report Statistic\", \"Value\"]) # method to create dataframe from report_list obtained from https://stackoverflow.com/questions/54992536/converting-a-list-of-dictionaries-to-dataframe\n",
        "        report_df.to_csv('/content/report.csv') # saves report dataframe into csv file named report.csv\n",
        "    def unique(self, list1): # method to pare down lists to their unique elements obtained from https://www.geeksforgeeks.org/python-get-unique-values-list/\n",
        "        list_set = set(list1) # converts a list into a set in order to make sure all items in the list are unique\n",
        "        return list(list_set) # converts the set of unique values stored in list_set back to a list\n",
        "    def tree_diagram(self): # method to create tree diagram of subfolders and files within folder_1\n",
        "        tree = Tree() # creates tree diagram to be populated\n",
        "        tree.create_node('folder_1', 'folder_1') # creates folder_1 node\n",
        "        tree.create_node('folder1_1', 'folder1_1', parent='folder_1') # creates folder1_1 node as child node to folder_1 parent node\n",
        "        tree.create_node('folder1_1_1', 'folder1_1_1', parent='folder1_1') # creates folder1_1_1 node as child node to folder1_1 parent node\n",
        "        tree.create_node('folder1_1_1_1', 'folder1_1_1_1', parent='folder1_1_1') # creates folder1_1_1_1 node as child node to folder1_1_1 parent node\n",
        "        tree.create_node('folder1_2', 'folder1_2', parent='folder_1') # creates folder1_2 node as child node to folder_1 parent node\n",
        "        tree.create_node('folder1_2_1', 'folder1_2_1', parent='folder1_2') # creates folder1_2_1 node as child node to folder1_2 parent node\n",
        "        tree.create_node('folder1_2_2', 'folder1_2_2', parent='folder1_2') # creates folder1_2_2 node as child node to folder1_2 parent node\n",
        "        tree.create_node('folder1_3', 'folder1_3', parent='folder_1') # creates folder1_3 node as child node to folder_1 parent node\n",
        "        for file in self.l1: # runs for first level files in folder_1\n",
        "            a = re.findall('[a-zA-Z0-9_-]+(?=)', file) # finds Filename and File Extension in folder_1 files using regex findall function and stores them in list a ([Filename, File Extension])\n",
        "            tree.create_node(a[0] + '.' + a[1], file, parent='folder_1') # creates child nodes from folder_1 files under folder_1 parent node, named by concatenating filename and file extension\n",
        "        for file in self.l2: # runs for second level files in folder_1\n",
        "            folder1_2 = 'folder1_2' # allows folder1_2 to be identified as a string\n",
        "            folder1_3 = 'folder1_3' # allows folder1_3 to be identified as string\n",
        "            if folder1_2 in file: # identifies if there are folder1_2 files\n",
        "                a = re.findall('[a-zA-Z0-9_-]+(?=)', file) # finds Folder, Filename and File Extension in folder1_2 files using regex findall function and stores them in list a ([Folder, Filename, File Extension])\n",
        "                tree.create_node(a[1] + '.' + a[2], file, parent='folder1_2') # creates child nodes from folder1_2 files under folder1_2 parent node, named by concatenating filename and file extension\n",
        "            if folder1_3 in file: # identifies if there are folder1_3 files\n",
        "                a = re.findall('[a-zA-Z0-9_-]+(?=)', file)  # finds Folder, Filename and File Extension in folder1_3 files using regex findall function and stores them in list a ([Folder, Filename, File Extension])\n",
        "                tree.create_node(a[1] + '.' + a[2], file, parent='folder1_3') # creates child nodes from folder1_3 files under folder1_3 parent node, named by concatenating filename and file extension\n",
        "        for file in self.l3: # runs for third level files in folder_1\n",
        "            folder1_1_1 = 'folder1_1_1' # allows folder1_1_1 to be identified as a string\n",
        "            folder1_2_1 = 'folder1_2_1' # allows folder1_2_1 to be identified as a string\n",
        "            folder1_2_2 = 'folder1_2_2' # allows folder1_3_1 to be identified as a string\n",
        "            if folder1_1_1 in file: # identifies if there are folder1_1_1 files\n",
        "                a = re.findall('[a-zA-Z0-9_-]+(?=)', file) # finds Folder, Subfolder, Filename and File Extension in folder1_1_1 files using regex findall function and stores them in list a ([Folder, Subfolder, Filename, File Extension])\n",
        "                tree.create_node(a[2] + '.' + a[3], file, parent='folder1_1_1') # creates child nodes from folder1_1_1 files under folder1_1_1 parent node, named by concatenating filename and file extension\n",
        "            if folder1_2_1 in file: # identifies if there are folder1_2_1 files\n",
        "                a = re.findall('[a-zA-Z0-9_-]+(?=)', file) # finds Folder, Subfolder, Filename and File Extension in folder1_2_1 files using regex findall function and stores them in list a ([Folder, Subfolder, Filename, File Extension])\n",
        "                tree.create_node(a[2] + '.' + a[3], file, parent='folder1_2_1') # creates child nodes from folder1_2_1 files under folder1_2_1 parent node, named by concatenating filename and file extension\n",
        "            if folder1_2_2 in file: # identifies if there are folder1_2_2 files\n",
        "                a = re.findall('[a-zA-Z0-9_-]+(?=)', file) # finds Folder, Subfolder, Filename and File Extension in folder1_2_2 files using regex findall function and stores them in list a ([Folder, Subfolder, Filename, File Extension])\n",
        "                tree.create_node(a[2] + '.' + a[3], file, parent='folder1_2_2') # creates child nodes from folder1_2_2 files under folder1_2_2 parent node, named by concatenating filename and file extension\n",
        "        for file in self.l4: # runs for fourth level files in folder_1\n",
        "            a = re.findall('[a-zA-Z0-9_-]+(?=)', file) # finds Folder, Subfolder, Subsubfolder, Filename and File Extension in folder1_1_1_1 files using regex findall function and stores them in list a ([Folder, Subfolder, Subsubfolder, Filename, File Extension])\n",
        "            tree.create_node(a[3] + '.' + a[4], file, parent='folder1_1_1_1') # creates child nodes from folder1_1_1_1 files under folder1_1_1_1 parent node, named by concatenating filename and file extension\n",
        "        tree.show() # shows the completed tree diagram\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder1 = FilePro(folder_1)"
      ],
      "metadata": {
        "id": "XszP14Yp00hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first method in FilePro is get_info. This method is used to identify specific characteristics of each file in folder_1. These characteristics are the extensions, sizes, MD5 checksum values, names, paths, and dates of modification for each file. In order to read each file more easily, the method separates folder_1 into four levels of organization using glob. Within each level, the characteristics are determined with various libraries. File extensions and names are determined using the regex findall function. File sizes are determined using the os path and getsize functions. MD5 checksum values are determined by reading each file in binary and using the hashlib md5 and hexdigest functions. Filepaths are determined by finding the path of the current working directory with the os get cwd function and concatenating them with the paths of each file. Dates of modification are determined by using the os path and getmtime functions, converting from timestamps to datetime using the datetime date and fromtimestamp functions, and converting from datetime to a date string using the datetime strftime function. It stores the file characteristics in dictionaries, which in turn are merged into a list. This list of dictionaries is then stored in a JSON file called file_info. The get_info method is also tied into the constructor method through a timing mechanism. If file_info has not been updated in two days or longer, then get_info overwrites the file with an updated version. Below, the functionality of get_info is displayed by opening file_info and reading each dictionary within it. The dictionaries within file_info that contain all the previously listed information are displayed below."
      ],
      "metadata": {
        "id": "onmFICsKzJXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder1.get_info()"
      ],
      "metadata": {
        "id": "3mWNExMJd2R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/file_info.json', 'r') as fp:\n",
        "    record_list = json.load(fp) # loads information from file_info as a list of file informatin dictionaries\n",
        "\n",
        "for item in record_list:\n",
        "    print(item) # prints the file information dictionaries"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsy13BjfwvDH",
        "outputId": "96d4715d-9f01-4ccc-95ac-7e8c4e693845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'File Extension': 'pdf', 'File Size': 192866, 'MD5 Checksum Value': '4b06ee27b38bb84d9a3bfa4a07082725', 'Filename': 'anderson_proofs', 'Filepath': '/content/drive/MyDrive/folder_1/anderson_proofs.pdf', 'Modified Date': '05/01/2018'}\n",
            "{'File Extension': 'pdf', 'File Size': 1439169, 'MD5 Checksum Value': '4794dd0c738a83654c8994d5defac72b', 'Filename': 'yang_baxter', 'Filepath': '/content/drive/MyDrive/folder_1/yang_baxter.pdf', 'Modified Date': '10/03/2018'}\n",
            "{'File Extension': 'pdf', 'File Size': 174641, 'MD5 Checksum Value': '765c27ee5332950b456af2cb30d8fc50', 'Filename': 'evolutionary_algorithm', 'Filepath': '/content/drive/MyDrive/folder_1/evolutionary_algorithm.pdf', 'Modified Date': '01/25/2019'}\n",
            "{'File Extension': 'pdf', 'File Size': 221003, 'MD5 Checksum Value': '29d6902ed62efe4c8a707129ea03bba4', 'Filename': 'borealis_algorithm', 'Filepath': '/content/drive/MyDrive/folder_1/borealis_algorithm.pdf', 'Modified Date': '09/14/2019'}\n",
            "{'File Extension': 'pdf', 'File Size': 2710116, 'MD5 Checksum Value': '9be7299cfd8450f6ea36653d5de2535d', 'Filename': 'Exact_NMF', 'Filepath': '/content/drive/MyDrive/folder_1/Exact_NMF.pdf', 'Modified Date': '02/16/2020'}\n",
            "{'File Extension': 'json', 'File Size': 995, 'MD5 Checksum Value': 'e47902d53af8aba16ded09d9cbea1a64', 'Filename': 'fixedparam_allres_res_neq_5_1_p', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/fixedparam_allres_res_neq_5_1_p.json', 'Modified Date': '04/23/2020'}\n",
            "{'File Extension': 'json', 'File Size': 1007, 'MD5 Checksum Value': 'a4e4ec3ea55394fb044a15574b8d7bed', 'Filename': 'fixedparam_allres_res_neq_5_2_p', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/fixedparam_allres_res_neq_5_2_p.json', 'Modified Date': '04/23/2020'}\n",
            "{'File Extension': 'json', 'File Size': 1000, 'MD5 Checksum Value': 'c0dadb8a19eb6196bf04a9db997aeb5b', 'Filename': 'fixedparam_allres_res_eq_5_0_p1', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/fixedparam_allres_res_eq_5_0_p1.json', 'Modified Date': '04/23/2020'}\n",
            "{'File Extension': 'json', 'File Size': 999, 'MD5 Checksum Value': '09cfd5429216a96e6d3195269589a684', 'Filename': 'fixedparam_allres_res_eq_5_1_p1', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/fixedparam_allres_res_eq_5_1_p1.json', 'Modified Date': '04/23/2020'}\n",
            "{'File Extension': 'json', 'File Size': 1016, 'MD5 Checksum Value': '90119a17bd33c921a58aa179951c2b4e', 'Filename': 'fixedparam_allres_res_eq_5_1_p3', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/fixedparam_allres_res_eq_5_1_p3.json', 'Modified Date': '04/23/2020'}\n",
            "{'File Extension': 'json', 'File Size': 999, 'MD5 Checksum Value': 'f9d101243319783beeac7bc9f902de54', 'Filename': 'fixedparam_allres_res_eq_5_1_p2', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/fixedparam_allres_res_eq_5_1_p2.json', 'Modified Date': '04/23/2020'}\n",
            "{'File Extension': 'json', 'File Size': 1001, 'MD5 Checksum Value': 'cacdf51728b140139da582bd9f87267e', 'Filename': 'fixedparam_allres_res_eq_5_0_p2', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/fixedparam_allres_res_eq_5_0_p2.json', 'Modified Date': '04/23/2020'}\n",
            "{'File Extension': 'json', 'File Size': 995, 'MD5 Checksum Value': 'dd0c3e133b002783ddc6523a67173730', 'Filename': 'fixedparam_allres_res_neq_5_0_p', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/fixedparam_allres_res_neq_5_0_p.json', 'Modified Date': '04/23/2020'}\n",
            "{'File Extension': 'json', 'File Size': 1009, 'MD5 Checksum Value': '75efba8c6b233323227f75fc476fbfcb', 'Filename': 'fixedparam_allres_res_eq_5_0_p3', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/fixedparam_allres_res_eq_5_0_p3.json', 'Modified Date': '04/23/2020'}\n",
            "{'File Extension': 'txt', 'File Size': 149062, 'MD5 Checksum Value': 'b14ec0312a6a17a5b98a70e44f1492c7', 'Filename': 'the-winters-tale_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/the-winters-tale_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 135081, 'MD5 Checksum Value': '7392e19e6fdcac6725826587b35ae7b5', 'Filename': 'richard-ii_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/richard-ii_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 56790, 'MD5 Checksum Value': '3902e958ddadd73b7d2a89136681b13d', 'Filename': 'venus-and-adonis_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/venus-and-adonis_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 147304, 'MD5 Checksum Value': '0b23c4f521180266f49a3477945c20ff', 'Filename': 'the-two-noble-kinsmen_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/the-two-noble-kinsmen_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 125304, 'MD5 Checksum Value': '852e32a0b698ad85e8b11fa778d7562d', 'Filename': 'the-merchant-of-venice_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/the-merchant-of-venice_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 115166, 'MD5 Checksum Value': '79c6ae66de9262dcc59d4fefea109cf5', 'Filename': 'timon-of-athens_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/timon-of-athens_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 166143, 'MD5 Checksum Value': 'b2429ac3cad1b21e565eec5de0d18222', 'Filename': 'troilus-and-cressida_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/troilus-and-cressida_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 181588, 'MD5 Checksum Value': '2332ead832617625bfdc886b68cd5000', 'Filename': 'richard-iii_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/richard-iii_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 113339, 'MD5 Checksum Value': '70e8ac7ca9965048d31286501b051a13', 'Filename': 'pericles_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/pericles_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 130330, 'MD5 Checksum Value': 'f254646f163b93a0966fe45fc200cfc8', 'Filename': 'the-taming-of-the-shrew_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/the-taming-of-the-shrew_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 159411, 'MD5 Checksum Value': '988bfdd6df9db1d39e1164ea8b9370cc', 'Filename': 'othello_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/othello_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 103322, 'MD5 Checksum Value': '137ed9d7f46f7b012c528776900095ce', 'Filename': 'the-tempest_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/the-tempest_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 136592, 'MD5 Checksum Value': 'e39a33c35eb050581ddf68610df66840', 'Filename': 'the-merry-wives-of-windsor_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/the-merry-wives-of-windsor_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 99272, 'MD5 Checksum Value': '25ec8c919e5ea70427b9839db7824d98', 'Filename': 'shakespeares-sonnets_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/shakespeares-sonnets_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 105727, 'MD5 Checksum Value': '5f0650443eb01e0f493ba922d77b0cfb', 'Filename': 'the-two-gentlemen-of-verona_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/the-two-gentlemen-of-verona_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 93250, 'MD5 Checksum Value': 'e1719a144cede587a8ca82b75c1e82a5', 'Filename': 'the-comedy-of-errors_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/the-comedy-of-errors_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 124759, 'MD5 Checksum Value': '81e4ae3f214ba4c5e16be8048cdba7d7', 'Filename': 'titus-andronicus_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/titus-andronicus_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 146706, 'MD5 Checksum Value': 'a5fdb7bc38b282456f1f4eba308c5ff0', 'Filename': 'romeo-and-juliet_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/romeo-and-juliet_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 2466, 'MD5 Checksum Value': 'b98bc859a2091515d4c683f5275ac3ec', 'Filename': 'the-phoenix-and-turtle_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/the-phoenix-and-turtle_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 119215, 'MD5 Checksum Value': '4103e15b1a1e9775cee6524841f005d5', 'Filename': 'twelfth-night_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_3/twelfth-night_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'csv', 'File Size': 36523880, 'MD5 Checksum Value': '0520d1f4f7f8eea21da078072d97aa8c', 'Filename': 'mnist_train_small', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/folder1_2_2/mnist_train_small.csv', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'csv', 'File Size': 18289443, 'MD5 Checksum Value': 'c807df8d6d804ab2647fc15c3d40f543', 'Filename': 'mnist_test', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/folder1_2_2/mnist_test.csv', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'csv', 'File Size': 1706430, 'MD5 Checksum Value': 'ec3a79a3216a5c438d21763350fa2e6c', 'Filename': 'california_housing_train', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/folder1_2_2/california_housing_train.csv', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'csv', 'File Size': 301141, 'MD5 Checksum Value': 'd9a4b24d17770fad3209a18a4a5a3750', 'Filename': 'california_housing_test', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/folder1_2_2/california_housing_test.csv', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'csv', 'File Size': 56890, 'MD5 Checksum Value': '8ebf54b2719a1473f154e8fd7bdde817', 'Filename': 'mlb_players', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/folder1_2_1/mlb_players.csv', 'Modified Date': '02/21/2019'}\n",
            "{'File Extension': 'csv', 'File Size': 591, 'MD5 Checksum Value': 'a868e2a3743e50f26765fd320e9e264a', 'Filename': 'plain_csv_file', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/folder1_2_1/plain_csv_file.csv', 'Modified Date': '04/19/2019'}\n",
            "{'File Extension': 'csv', 'File Size': 4413, 'MD5 Checksum Value': 'd509bc1f1a41bc9d239f1aaaccf42354', 'Filename': 'oscar_age_male', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/folder1_2_1/oscar_age_male.csv', 'Modified Date': '04/19/2019'}\n",
            "{'File Extension': 'csv', 'File Size': 4438, 'MD5 Checksum Value': 'cbe679301021cb1f5bf26372a263a0e5', 'Filename': 'oscar_age_female', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/folder1_2_1/oscar_age_female.csv', 'Modified Date': '04/19/2019'}\n",
            "{'File Extension': 'json', 'File Size': 76611, 'MD5 Checksum Value': '716415559dc056e3e4384e9618929d03', 'Filename': 'example', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/folder1_2_1/example.json', 'Modified Date': '04/20/2019'}\n",
            "{'File Extension': 'json', 'File Size': 206, 'MD5 Checksum Value': '0c9685784f7c96a2d34217f367eea5c8', 'Filename': 'geo_data', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_2/folder1_2_1/geo_data.json', 'Modified Date': '04/20/2019'}\n",
            "{'File Extension': 'txt', 'File Size': 152246, 'MD5 Checksum Value': 'aa1059eca01be406763ef19a8920569d', 'Filename': 'henry-vi-part-3_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/henry-vi-part-3_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 136156, 'MD5 Checksum Value': 'a6f85bb9a420228b48cde43d62a59bbe', 'Filename': 'henry-vi-part-1_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/henry-vi-part-1_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 149163, 'MD5 Checksum Value': 'e74d10aa470e0143a9f572772e32b1ea', 'Filename': 'henry-viii_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/henry-viii_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 157464, 'MD5 Checksum Value': '0ebe6cd7cae167074f0be80765ead4d5', 'Filename': 'henry-v_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/henry-v_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 145915, 'MD5 Checksum Value': '2638b75f559b5549c1d94ca0d087f52d', 'Filename': 'henry-iv-part-1_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/henry-iv-part-1_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 158182, 'MD5 Checksum Value': '1442a4652bdd8df0da28fd6f39b10cf9', 'Filename': 'henry-vi-part-2_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/henry-vi-part-2_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'txt', 'File Size': 159858, 'MD5 Checksum Value': '2708a688ef8119f528bfc46a4d174aaa', 'Filename': 'henry-iv-part-2_TXT_FolgerShakespeare', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/henry-iv-part-2_TXT_FolgerShakespeare.txt', 'Modified Date': '10/07/2022'}\n",
            "{'File Extension': 'pdf', 'File Size': 660458, 'MD5 Checksum Value': '01ec1fe4f0d71404de00a4703adb8cd1', 'Filename': 'QUEST', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/folder1_1_1_1/QUEST.pdf', 'Modified Date': '06/18/2022'}\n",
            "{'File Extension': 'pdf', 'File Size': 244768, 'MD5 Checksum Value': '6c5d4a57d764a1386e1dbe73d12b6564', 'Filename': 'OPTIC_binary_classifier', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/folder1_1_1_1/OPTIC_binary_classifier.pdf', 'Modified Date': '06/26/2022'}\n",
            "{'File Extension': 'pdf', 'File Size': 2543515, 'MD5 Checksum Value': '86abc802fe679c5e6c6e080ae2b57869', 'Filename': 'quantum_image_rep_survey', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/folder1_1_1_1/quantum_image_rep_survey.pdf', 'Modified Date': '07/20/2022'}\n",
            "{'File Extension': 'pdf', 'File Size': 796505, 'MD5 Checksum Value': 'bdc456275be3fd4a4e1a739b02c5ecc7', 'Filename': 'qualpi_image_reg', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/folder1_1_1_1/qualpi_image_reg.pdf', 'Modified Date': '07/21/2022'}\n",
            "{'File Extension': 'pdf', 'File Size': 429098, 'MD5 Checksum Value': '60c2c9a631b297f23952d3fb00f3bf10', 'Filename': 'quantum_branch_and_bound', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/folder1_1_1_1/quantum_branch_and_bound.pdf', 'Modified Date': '07/25/2022'}\n",
            "{'File Extension': 'exe', 'File Size': 8849459, 'MD5 Checksum Value': 'cd53853927ccb903d35884fd096616e5', 'Filename': 'basic_program', 'Filepath': '/content/drive/MyDrive/folder_1/folder1_1/folder1_1_1/folder1_1_1_1/basic_program.exe', 'Modified Date': '09/09/2022'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second method in FilePro is generate_report. It uses the previous method's file_info JSON file to create a statistical report. This statistical report is  focused on comparisons related to the file and folder sizes within folder_1. The report contains mean and median file sizes for all files within folder_1, and also contains these statistics for each particular file extension. The means and medians are determined using numpy's mean and median functions. The organization of each unique file extension that allows for their mean and median sizes to be determined is performed by a sourced method called unique. This method converts lists to sets (which remove repeated elements) and converts the sets of unique elements back to lists. Thus, each file extension becomes a single dictionary key, and all values for each file extension become a list of dictionary values for their specified keys. Additionally, the largest folders by both number of files and combined file size are determined and displayed in the report. These are determined by performing similar methods to the organization of each file extension and their size. Each folder is organized into dictionaries with their unique names (derived from the unique method) as keys and either the file names or file sizes as the list of values. The number of file statistics is obtained by determining the length of each list of filenames in unique folders, and the combined file size is determined by summing the list of file sizes in unique folders. The maximum values for each are determined by performing the max function on each unique folder and determining the key associated with each max value. Each statistic is saved in a dictionary with their key identifying the type of statistic and their value indicating their numerical value. The report is created by saving each statistic into a dictionary, combining the dictionaries into a list, converting the list of dictionaries into a pandas data frame, and saving the data frame to a csv file called report. The generated report is displayed below.\n"
      ],
      "metadata": {
        "id": "An-J_wbq1fa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder1.generate_report()"
      ],
      "metadata": {
        "id": "CO3KMjFAFtWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('/content/report.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Sih611oiwGt8",
        "outputId": "41db32ea-3b73-48c7-f173-29f4f01bc473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Unnamed: 0                      Report Statistic  \\\n",
              "0            0     Mean Size of All Files (in bytes)   \n",
              "1            1   Median Size of All Files (in bytes)   \n",
              "2            2              pdf Mean Size (in bytes)   \n",
              "3            3            pdf Median Size (in bytes)   \n",
              "4            4              csv Mean Size (in bytes)   \n",
              "5            5            csv Median Size (in bytes)   \n",
              "6            6              exe Mean Size (in bytes)   \n",
              "7            7            exe Median Size (in bytes)   \n",
              "8            8             json Mean Size (in bytes)   \n",
              "9            9           json Median Size (in bytes)   \n",
              "10          10              txt Mean Size (in bytes)   \n",
              "11          11            txt Median Size (in bytes)   \n",
              "12          12     Largest Folder by Number of Files   \n",
              "13          13  Largest Folder by Combined File Size   \n",
              "\n",
              "                                                Value  \n",
              "0                                  1380780.2280701755  \n",
              "1                                            136156.0  \n",
              "2                                            941213.9  \n",
              "3                                            544778.0  \n",
              "4                                          7110903.25  \n",
              "5                                            179015.5  \n",
              "6                                           8849459.0  \n",
              "7                                           8849459.0  \n",
              "8                                   7803.454545454545  \n",
              "9                                              1000.0  \n",
              "10                                 128511.51851851853  \n",
              "11                                           136156.0  \n",
              "12          /content/drive/MyDrive/folder_1/folder1_3  \n",
              "13  /content/drive/MyDrive/folder_1/folder1_2/fold...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c790d141-7027-473d-9888-a3ad278e6741\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Report Statistic</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Mean Size of All Files (in bytes)</td>\n",
              "      <td>1380780.2280701755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Median Size of All Files (in bytes)</td>\n",
              "      <td>136156.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>pdf Mean Size (in bytes)</td>\n",
              "      <td>941213.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>pdf Median Size (in bytes)</td>\n",
              "      <td>544778.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>csv Mean Size (in bytes)</td>\n",
              "      <td>7110903.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>csv Median Size (in bytes)</td>\n",
              "      <td>179015.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>exe Mean Size (in bytes)</td>\n",
              "      <td>8849459.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>exe Median Size (in bytes)</td>\n",
              "      <td>8849459.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>json Mean Size (in bytes)</td>\n",
              "      <td>7803.454545454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>json Median Size (in bytes)</td>\n",
              "      <td>1000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>txt Mean Size (in bytes)</td>\n",
              "      <td>128511.51851851853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>txt Median Size (in bytes)</td>\n",
              "      <td>136156.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>Largest Folder by Number of Files</td>\n",
              "      <td>/content/drive/MyDrive/folder_1/folder1_3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>Largest Folder by Combined File Size</td>\n",
              "      <td>/content/drive/MyDrive/folder_1/folder1_2/fold...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c790d141-7027-473d-9888-a3ad278e6741')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c790d141-7027-473d-9888-a3ad278e6741 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c790d141-7027-473d-9888-a3ad278e6741');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The third method in FilePro is tree_diagram. This method uses the treelib library to create a tree diagram of folder_1. Each folder within folder_1 is created as a node using the treelib create_node function, and directed to the right layout using parent nodes. The files are then populated into the proper folders using the same levels of organization from glob as get_info, and specifying their folder targets. The tree diagram is then created, displaying the layout of the files and subfolders within folder_1 in a clear and organized visual. The diagram is displayed below."
      ],
      "metadata": {
        "id": "XruXjQ1v4vB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder1.tree_diagram()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--55tRZ6TPCb",
        "outputId": "9a96e9cd-32d7-48f2-9573-d0b1d0a79237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "folder_1\n",
            " Exact_NMF.pdf\n",
            " anderson_proofs.pdf\n",
            " borealis_algorithm.pdf\n",
            " evolutionary_algorithm.pdf\n",
            " folder1_1\n",
            "    folder1_1_1\n",
            "        folder1_1_1_1\n",
            "           OPTIC_binary_classifier.pdf\n",
            "           QUEST.pdf\n",
            "           basic_program.exe\n",
            "           qualpi_image_reg.pdf\n",
            "           quantum_branch_and_bound.pdf\n",
            "           quantum_image_rep_survey.pdf\n",
            "        henry-iv-part-1_TXT_FolgerShakespeare.txt\n",
            "        henry-iv-part-2_TXT_FolgerShakespeare.txt\n",
            "        henry-v_TXT_FolgerShakespeare.txt\n",
            "        henry-vi-part-1_TXT_FolgerShakespeare.txt\n",
            "        henry-vi-part-2_TXT_FolgerShakespeare.txt\n",
            "        henry-vi-part-3_TXT_FolgerShakespeare.txt\n",
            "        henry-viii_TXT_FolgerShakespeare.txt\n",
            " folder1_2\n",
            "    fixedparam_allres_res_eq_5_0_p1.json\n",
            "    fixedparam_allres_res_eq_5_0_p2.json\n",
            "    fixedparam_allres_res_eq_5_0_p3.json\n",
            "    fixedparam_allres_res_eq_5_1_p1.json\n",
            "    fixedparam_allres_res_eq_5_1_p2.json\n",
            "    fixedparam_allres_res_eq_5_1_p3.json\n",
            "    fixedparam_allres_res_neq_5_0_p.json\n",
            "    fixedparam_allres_res_neq_5_1_p.json\n",
            "    fixedparam_allres_res_neq_5_2_p.json\n",
            "    folder1_2_1\n",
            "       example.json\n",
            "       geo_data.json\n",
            "       mlb_players.csv\n",
            "       oscar_age_female.csv\n",
            "       oscar_age_male.csv\n",
            "       plain_csv_file.csv\n",
            "    folder1_2_2\n",
            "        california_housing_test.csv\n",
            "        california_housing_train.csv\n",
            "        mnist_test.csv\n",
            "        mnist_train_small.csv\n",
            " folder1_3\n",
            "    othello_TXT_FolgerShakespeare.txt\n",
            "    pericles_TXT_FolgerShakespeare.txt\n",
            "    richard-ii_TXT_FolgerShakespeare.txt\n",
            "    richard-iii_TXT_FolgerShakespeare.txt\n",
            "    romeo-and-juliet_TXT_FolgerShakespeare.txt\n",
            "    shakespeares-sonnets_TXT_FolgerShakespeare.txt\n",
            "    the-comedy-of-errors_TXT_FolgerShakespeare.txt\n",
            "    the-merchant-of-venice_TXT_FolgerShakespeare.txt\n",
            "    the-merry-wives-of-windsor_TXT_FolgerShakespeare.txt\n",
            "    the-phoenix-and-turtle_TXT_FolgerShakespeare.txt\n",
            "    the-taming-of-the-shrew_TXT_FolgerShakespeare.txt\n",
            "    the-tempest_TXT_FolgerShakespeare.txt\n",
            "    the-two-gentlemen-of-verona_TXT_FolgerShakespeare.txt\n",
            "    the-two-noble-kinsmen_TXT_FolgerShakespeare.txt\n",
            "    the-winters-tale_TXT_FolgerShakespeare.txt\n",
            "    timon-of-athens_TXT_FolgerShakespeare.txt\n",
            "    titus-andronicus_TXT_FolgerShakespeare.txt\n",
            "    troilus-and-cressida_TXT_FolgerShakespeare.txt\n",
            "    twelfth-night_TXT_FolgerShakespeare.txt\n",
            "    venus-and-adonis_TXT_FolgerShakespeare.txt\n",
            " yang_baxter.pdf\n",
            "\n"
          ]
        }
      ]
    }
  ]
}